# Chapter 2: regression and model validation

*Goal of this project is to practice regression and model validation with R on a dataset which is saved in the directory IODS-project under the name "learning2014". The dataset is obtained from the first part of the RStudio Exercise 2, with the code available at [my Github repository](https://github.com/scarabel/IODS-project/blob/master/data/create_learning2014.R).*

What I learnt:

* to produce summaries and graphical representations of the data
* to fit a linear regression model to the data
* to validate the model with graphical tools


## The database
I first load the dataset from the folder IODS-project and save it with name "students2014".
```{r}
setwd("~/GitHub/IODS-project")
options(max.print=2000)
students2014 <- read.csv("learning2014")
```

The structure of the database can be explored with the instruction ```str```:
```{r} 
str(students2014)
```

The database contains a summary of the information about statistical learning collected by Kimmo Venhalahti during the period 3.12.2014 - 10.1.2015, from the students attnding the course *Johdatus yhteiskuntatilastotieteeseen*.
The full information was collected and summarised in the following fields:

* age = age in years;
* gender = gender of the students (M or F);
* attitude = attitude towards statistics;
* deep = summary of the variables characterising deep learning;
* stra = summary of the variables characterising strategic learning;
* surf = summary of the variables characterising surface learining;
* points = exam grade.

The database has 166 entries containing the data for every students who got a positive final grade.

## Graphical overview and summary of the data
I first give an overview of the age and gender of the interviewed students.
```{r}
summary(students2014$age)
summary(students2014$gender)
```

To explore the relations between the variables, I create a scattered plot of the different variables, keeping track of the gender.
To plot the results, I first load the library ```ggplot2``` and ```GGally```

```{r eval=TRUE, echo=FALSE}
library(GGally)
library(ggplot2)
p <- ggpairs(students2014[-1], mapping = aes(col=gender), lower = list(combo = wrap("facethist", bins = 20)))
p
```

Some remarks about the results

* the shape of all distributions (but for points) resemble a Gaussian distribution;
* males seem to show a higher attitude towards statistics than female; 
* age does not seem to influence the attitude nor the type of learning;
* deep learning seems to be very little correlated with the exam points;
* some correlation can be identified (also quantitavely), between the exam points and attitude, strategic learning and surface learning. 

I will explore graphically specifically the correlation between points and attitude, strategic learning and surface learning.
In particular, below I repeat the graphic relation between the variables, including their linear regression line fitted to the data.

```{r eval=TRUE, echo=FALSE, fig.width=3, fig.height=3}
qplot(attitude, points, data = students2014) + geom_smooth(method = "lm") + ggtitle("Points versus attitude")
qplot(stra, points, data = students2014) + geom_smooth(method = "lm") + ggtitle("Points versus strategic learning")
qplot(surf, points, data = students2014) + geom_smooth(method = "lm") + ggtitle("Points versus surface learning")
```

## Fitting a statistical linear regression model
I now fit a regression model for the target variable *point*, with explanatory variables $x_1=$ *attitude*, $x_2=$ *strategic learning* and $x_3=$ *surface learning*.

```{r eval=TRUE}
my_model <- lm(points ~ attitude + stra + surf, data = students2014)
summary(my_model)
```

The summary of the statistical model provides the coefficients of the multiple linear regression model for the explanatory variables $x_1,x_2,x_3$. The coefficients are $\beta_1 \approx 0.33, \beta_2 \approx 0.85, \beta_3 \approx -0.58$.

To understand the significance of the model, we look at the p-value, ```Pr(>|t|)```. We observe that the p-value corresponding to the surface learning is very high, so this variable is probably not relevant in explaining the exam points.

We therefore take it out the model and consider only the first two explanatory variables.

```{r eval=TRUE}
my_model2 <- lm(points ~ attitude + stra, data = students2014)
summary(my_model2)
```

From the new p-values, we notice that both the variables have a high significance in explaining the variable *points*.

## Interpretation of the regression coefficients
In this new model, the coefficients of the explanatory variables are $\beta_1\approx 0.34$ and $\beta_2\approx 0.91$. This means that for any unitary increase of attitude or strategic learning, the points are expected to increase of 0.34 or 0.91, respectively.
Hence, the strategic learning seems to have a higher effect on the exam points.

The value of $R^2$ indicates how well the data are explained by the linear regression model. A value of 0.2 indicates that the data are quite scattered away from the regression line (i.e., the dispersion of the data is quite high). Compare also the previous plots of points vs attitude and points vs strategic learning.

## Graphical model validation
To understand how reasonable is the assumption $\epsilon \sim N(0,\sigma^2)$, I plot the diagnostic plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.

```{r eval=TRUE, echo=FALSE, fig.width=9, fig.height=3}
par(mfrow = c(1,3))
plot(my_model2, which=c(1,2,5))
```

* From the first plot, there seems to be no pattern between data and residuals. Hence it is reasonable to assume that the noise $\epsilon$ is independent of the data (i.e., the variance $\sigma^2$ is constant).

* The second plot shows that the stardardised residuals fit well with the straight line: the normality assumption is reasonable.

* The third plot shows that there are no important outliers in the data that have high impact in the fitting of the model. Notice the small values of the leverage in the $x$-axis.








